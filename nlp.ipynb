{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hsichengyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hsichengyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "SPECIAL_CHARS = '[^A-Za-z0-9 ]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized = [word for sent in [re.sub(SPECIAL_CHARS, '', element).split(' ') for \n",
    "                                   element in nltk.sent_tokenize(text)] for word in sent]\n",
    "    lowered = [word.lower() for word in tokenized]\n",
    "    return lowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list()\n",
    "for ix in range(dataset.shape[0]):\n",
    "    content = list()\n",
    "    for e in dataset.iloc[ix,4:9]:\n",
    "        content.append(str(e))\n",
    "    s = ','.join(content)\n",
    "    ls.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['everything'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_texts = [preprocess(text) for text in dataset['everything'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take out the stopwords\n",
    "# for i in range(len(description_texts)):\n",
    "#     description_texts[i] = [word for word in description_texts[i] if word not in stopwords.words('english')]\n",
    "\n",
    "# I use movie title as unique key. So I map out the title and the tokenised sentences\n",
    "\n",
    "title_text = dict(zip(dataset['title'].tolist(), description_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(min_count=20,\n",
    "                window=2,\n",
    "                size=300,\n",
    "                sample=6e-5,\n",
    "                alpha=0.03,\n",
    "                min_alpha=0.0007,\n",
    "                negative=20,\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(first_map, second_map):\n",
    "    first_vec  = dict()\n",
    "    for uid, content in first_map.items():\n",
    "        temp = list()\n",
    "        for element in content:\n",
    "            try:\n",
    "                temp.append(second_map[element])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first_vec[uid] = np.mean(temp, axis=0)\n",
    "    \n",
    "    return first_vec\n",
    "\n",
    "\n",
    "def get_most_similar(lookup_id):\n",
    "\n",
    "    sim = list()\n",
    "    \n",
    "    lookup_map = title_vec\n",
    "    subject_map = title_vec\n",
    "#     else:\n",
    "#         raise ValueError('Invalid value for parameter kind.')\n",
    "        \n",
    "    for uid, vec in lookup_map.items():\n",
    "        thisSim = cosine_similarity(vec.reshape(1, -1), subject_map[lookup_id].reshape(1, -1))\n",
    "        sim.append((uid, thisSim[0][0]))\n",
    "\n",
    "    return sorted(sim, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "def top_10_similar(title):\n",
    "    \n",
    "    x = get_most_similar(title)[1:11]\n",
    "#     for e in x:\n",
    "# #         rating = dataset.loc[dataset['title']==e[0]]['rating_value'].values.tolist()[0]\n",
    "# #         votes = dataset.loc[dataset['title']==e[0]]['votes'].values.tolist()[0]\n",
    "# #         print(f\"Movie title: {e[0]}\\nScores: {rating}\\nVotes: {votes}\\nSimilarity: {e[1]}\\n\")\n",
    "#         genres = test.loc[test['title']==e[0]['genres'].values.tolist()]\n",
    "#         print(f\"Movie title: {e[0]}\\nGenres: {genres}\\nSimilarity: {e[1]}\\n\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vec = get_vectors(title_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_similar('House of Cards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test 6k dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6k = pd.read_csv('netflixMovieDb.csv')\n",
    "data6k = data6k.append(pd.read_csv('netflixTvshowDb.csv')).append(pd.read_csv('dMoviesDb.csv')).append(pd.read_csv('dTvshowsDb.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list()\n",
    "for ix in range(data6k.shape[0]):\n",
    "    content = list()\n",
    "    for e in data6k.iloc[ix,4:9]:\n",
    "        content.append(str(e))\n",
    "    s = ','.join(content)\n",
    "    ls.append(s)\n",
    "    \n",
    "data6k['everything'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_texts = [preprocess(text) for text in data6k['everything'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out the stopwords\n",
    "for i in range(len(description_texts)):\n",
    "    description_texts[i] = [word for word in description_texts[i] if word not in stopwords.words('english')]\n",
    "\n",
    "# I use movie title as unique key. So I map out the title and the tokenised sentences\n",
    "\n",
    "title_text = dict(zip(data6k['title'].tolist(), description_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(description_texts, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vec = get_vectors(title_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_similar('Ultraman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_similar('House of Cards')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data6k.iloc[0, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Framed in the 1940s for the double murder of his wife and her lover, upstanding banker Andy Dufresne begins a new life at the Shawshank prison, where he puts his accounting skills to work for an amoral warden. During his long stretch in prison, Dufresne comes to be admired by the other inmates -- including an older prisoner named Red -- for his integrity and unquenchable sense of hope.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk.text, chunk.root.text, chunk.root.dep_, chunk.root.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.serve(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTv = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_li = nTv.loc[nTv['type']=='tvshow'].title.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tv_li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nTv.loc[nTv['title']=='The Witcher'].description.values.tolist()[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
