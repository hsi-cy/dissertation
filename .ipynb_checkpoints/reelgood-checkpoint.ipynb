{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_parameters = {\n",
    "    'year_end':2019,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://reelgood.com/movies/source/netflix\"\n",
    "re_url = requests.get(url, my_parameters, headers=HEADERS).url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(re_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.find_element_by_css_selector(\"#app_mountpoint > div:nth-child(4) > main > div:nth-child(6) > a > div > button\").click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "        driver.find_element_by_css_selector(\"#app_mountpoint > div:nth-child(4) > main > div:nth-child(6) > a > div > button\").click()\n",
    "        sleep(1)\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "offset max = 2000 for tvshow <br>\n",
    "-> get the links of every tvshows DONE<br>\n",
    "-> get the tvshows info DONE<br>\n",
    "-> get the links of every movies <br>\n",
    "-> get the movies info <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tv links\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "tvlinks = list()\n",
    "rows = soup.findAll('tr',{'class':'css-gfsdx9'})\n",
    "for row in rows:\n",
    "    tvlinks.append(row.find('td',{'class':'css-1u7zfla e126mwsw1'}).find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 存檔\n",
    "# tvlinks_rg = pd.DataFrame()\n",
    "# tvlinks_rg['tvlinks'] = tvlinks\n",
    "# tvlinks_rg.to_csv('tvlinks_rg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movies links\n",
    "soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "movielinks = list()\n",
    "rows = soup.findAll('tr',{'class':'css-o6sgwe'})\n",
    "for row in rows:\n",
    "    movielinks.append(row.find('td',{'class':'css-1u7zfla e126mwsw1'}).find('a')['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "r = requests.get(\"https://reelgood.com/movie/inception-2010\")\n",
    "soup2 = BeautifulSoup(r.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "title = soup2.find('h1',{'itemprop':'name'}).text\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "info_rows = soup2.find('div',{'class':'css-1ss0qk ey4ir3j0'}).findAll('span',{'class':'css-ee2w7g ey4ir3j1'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imdb_rating = info_rows[0].find('div').find('div').text\n",
    "imdb_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tomato_rating = info_rows[1].find('span').text\n",
    "tomato_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "genres = [genre.text for genre in info_rows[2].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "maturity_rating = info_rows[3].text\n",
    "maturity_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "time_period = info_rows[4].text\n",
    "time_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "season_num = info_rows[5].text\n",
    "season_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# status = info_rows[6].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "platform = [platform.text for platform in info_rows[6].findAll('a')]\n",
    "platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "tags = [tag.text for tag in info_rows[7].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "country = [country.text for country in info_rows[8].findAll('a')]\n",
    "country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "description = soup2.find('p',{'itemprop':'description'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_movieinfo_rg(links):\n",
    "    start = timeit.default_timer()\n",
    "    content_type = 'movie'\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'imdb_rating', 'tomato_rating', \n",
    "                               'genres', 'maturity_rating', 'time_period', 'runtime',\n",
    "                               'platform', 'tags', 'country','description','ps'])\n",
    "    count = 1\n",
    "    for ix in range(len(links)):\n",
    "#         print(count)\n",
    "        if count % 25 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        \n",
    "        full_url = 'https://reelgood.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        try:\n",
    "            title = soup.find('h1',{'itemprop':'name'}).text\n",
    "        except:\n",
    "            title = ''\n",
    "        info_rows = soup.find('div',{'class':'css-1ss0qk ey4ir3j0'}).findAll('span',{'class':'css-ee2w7g ey4ir3j1'})\n",
    "        if len(info_rows) == 9:\n",
    "            try:\n",
    "                imdb_rating = info_rows[0].find('div').find('div').text\n",
    "            except:\n",
    "                imdb_rating = ''\n",
    "\n",
    "            try:\n",
    "                tomato_rating = info_rows[1].find('span').text\n",
    "            except:\n",
    "                tomato_rating = ''\n",
    "\n",
    "            try:\n",
    "                genres = [genre.text for genre in info_rows[2].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "            except:\n",
    "                genres = [genre.text for genre in info_rows[2].findAll('a')]\n",
    "            maturity_rating = info_rows[3].text\n",
    "            time_period = info_rows[4].text\n",
    "            runtime = info_rows[5].text\n",
    "            platform = [platform.text for platform in info_rows[6].findAll('a')]\n",
    "\n",
    "            try:\n",
    "                tags = [tag.text for tag in info_rows[7].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "            except:\n",
    "                tags = [tag.text for tag in info_rows[7].findAll('a')]\n",
    "            country = [country.text for country in info_rows[8].findAll('a')]\n",
    "            ps = 'All Clear'\n",
    "        else:\n",
    "            imdb_rating = ''\n",
    "            tomato_rating = ''\n",
    "            genres = ''\n",
    "            maturity_rating = ''\n",
    "            time_period = ''\n",
    "            runtime = ''\n",
    "            platform = ''\n",
    "            tags = ''\n",
    "            country = ''\n",
    "            ps = [info_rows[ix].text for ix in range(len(info_rows))]    \n",
    "            \n",
    "            \n",
    "        description = soup.find('p',{'itemprop':'description'}).text\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'imdb_rating', 'tomato_rating', \n",
    "                               'genres', 'maturity_rating', 'time_period', 'runtime',\n",
    "                               'platform', 'tags', 'country','description','ps'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'imdb_rating': imdb_rating,\n",
    "                             'tomato_rating': tomato_rating,\n",
    "                             'genres': genres,\n",
    "                             'maturity_rating': maturity_rating,\n",
    "                             'time_period': time_period,\n",
    "                             'runtime': runtime,\n",
    "                             \n",
    "                             'platform': platform,\n",
    "                             'tags': tags,\n",
    "                             'country': country,\n",
    "                             'description':description,\n",
    "                             'ps':ps\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        sleep(0.5)\n",
    "    stop = timeit.default_timer()\n",
    "    t = int((stop-start)/60)\n",
    "    print(f\"It took {t} minutes to scrape rating values and votes.\")    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movie info ttl 3749 \n",
    "movielinks = pd.read_csv('movielinks_rg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlinks = movielinks.movielinks_rg.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 11 minutes to scrape rating values and votes.\n",
      "200\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 11 minutes to scrape rating values and votes.\n",
      "300\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "400\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "500\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "600\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "700\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "800\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "900\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1000\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1100\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1200\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1300\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1400\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "1500\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1600\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "1700\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1800\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "1900\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "2000\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "2100\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "2200\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 11 minutes to scrape rating values and votes.\n",
      "2300\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "2400\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "2500\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "2600\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 11 minutes to scrape rating values and votes.\n",
      "2700\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "2800\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "2900\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "3000\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "3100\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "3200\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "3300\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "3400\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 9 minutes to scrape rating values and votes.\n",
      "3500\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "3600\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 10 minutes to scrape rating values and votes.\n",
      "3700\n",
      "25\n",
      "50\n",
      "75\n",
      "100\n",
      "It took 7 minutes to scrape rating values and votes.\n"
     ]
    }
   ],
   "source": [
    "# get 100 at a time just to be save MOVIE\n",
    "ix = 100\n",
    "\n",
    "for i in range(37): #just run 37 times. The rest 49 i will get by other codes\n",
    "    print(ix)\n",
    "    try:\n",
    "        if i == 0:\n",
    "            nMovieRg = get_movieinfo_rg(mlinks[0:ix]) \n",
    "            nMovieRg.to_csv('nMovieRg.csv')\n",
    "            ix += 100\n",
    "        else:\n",
    "            temp = get_movieinfo_rg(mlinks[(ix-100):ix])\n",
    "            nMovieRg = nMovieRg.append(temp, ignore_index=True)\n",
    "            nMovieRg.to_csv('nMovieRg.csv')\n",
    "            ix += 100 \n",
    "    except:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('nMovieRg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3700, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info_rg(content_type, links):\n",
    "    start = timeit.default_timer()\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'imdb_rating', 'tomato_rating', \n",
    "                               'genres', 'maturity_rating', 'time_period', 'season_num',\n",
    "                               'status', 'platform', 'tags', 'country','description','ps'])\n",
    "    count = 1\n",
    "    for ix in range(len(links)):\n",
    "#         print(count)\n",
    "        if count % 25 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        \n",
    "        full_url = 'https://reelgood.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1',{'itemprop':'name'}).text\n",
    "        \n",
    "        info_rows = soup.find('div',{'class':'css-1ss0qk ey4ir3j0'}).findAll('span',{'class':'css-ee2w7g ey4ir3j1'})\n",
    "        if len(info_rows) == 10:\n",
    "            try:\n",
    "                imdb_rating = info_rows[0].find('div').find('div').text\n",
    "            except:\n",
    "                imdb_rating = ''\n",
    "\n",
    "            try:\n",
    "                tomato_rating = info_rows[1].find('span').text\n",
    "            except:\n",
    "                tomato_rating = ''\n",
    "\n",
    "            try:\n",
    "                genres = [genre.text for genre in info_rows[2].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "            except:\n",
    "                genres = [genre.text for genre in info_rows[2].findAll('a')]\n",
    "            maturity_rating = info_rows[3].text\n",
    "            time_period = info_rows[4].text\n",
    "            season_num = info_rows[5].text\n",
    "            status = info_rows[6].text\n",
    "            platform = [platform.text for platform in info_rows[7].findAll('a')]\n",
    "\n",
    "            try:\n",
    "                tags = [tag.text for tag in info_rows[8].find('div',{'aria-hidden':'true'}).findAll('a')]\n",
    "            except:\n",
    "                tags = [tag.text for tag in info_rows[8].findAll('a')]\n",
    "            country = [country.text for country in info_rows[9].findAll('a')]\n",
    "            ps = 'All Clear'\n",
    "        else:\n",
    "            imdb_rating = ''\n",
    "            tomato_rating = ''\n",
    "            genres = ''\n",
    "            maturity_rating = ''\n",
    "            time_period = ''\n",
    "            season_num = ''\n",
    "            status = ''\n",
    "            platform = ''\n",
    "            tags = ''\n",
    "            country = ''\n",
    "            ps = [info_rows[ix].text for ix in range(len(info_rows))]    \n",
    "            \n",
    "            \n",
    "        description = soup.find('p',{'itemprop':'description'}).text\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'imdb_rating', 'tomato_rating', \n",
    "                               'genres', 'maturity_rating', 'time_period', 'season_num',\n",
    "                               'status', 'platform', 'tags', 'country','description','ps'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'imdb_rating': imdb_rating,\n",
    "                             'tomato_rating': tomato_rating,\n",
    "                             'genres': genres,\n",
    "                             'maturity_rating': maturity_rating,\n",
    "                             'time_period': time_period,\n",
    "                             'season_num': season_num,\n",
    "                             'status': status,\n",
    "                             'platform': platform,\n",
    "                             'tags': tags,\n",
    "                             'country': country,\n",
    "                             'description':description,\n",
    "                             'ps':ps\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        sleep(0.5)\n",
    "    stop = timeit.default_timer()\n",
    "    t = int((stop-start)/60)\n",
    "    print(f\"It took {t} minutes to scrape rating values and votes.\")    \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the first 100\n",
    "ix = 100\n",
    "# nTvshowRg = pd.DataFrame()\n",
    "for i in range(1:19):\n",
    "    print(ix)\n",
    "    try:\n",
    "        if i == 0:\n",
    "            nTvshowRg = get_info_rg('tvshow', tvlinks[0:ix]) \n",
    "            nTvshowRg.to_csv('nTvshowRg.csv')\n",
    "            ix += 100\n",
    "        else:\n",
    "            temp = get_info_rg('tvshow', tvlinks[(ix-100):ix])\n",
    "            nTvshowRg = nTvshowRg.append(temp, ignore_index=True)\n",
    "            nTvshowRg.to_csv('nTvshowRg.csv')\n",
    "            ix += 100\n",
    "    except:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 100 has been saved\n",
    "ix = 200\n",
    "for i in range(18):\n",
    "    print(ix)\n",
    "    try:\n",
    "        temp = get_info_rg('tvshow', tvlinks[(ix-100):ix])\n",
    "        nTvshowRg = nTvshowRg.append(temp, ignore_index=True)\n",
    "        nTvshowRg.to_csv('nTvshowRg.csv')\n",
    "        ix += 100\n",
    "    except:\n",
    "        print(i)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = pd.read_csv('nTvshowRg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTvshowRg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_spans = list()\n",
    "count = 1\n",
    "for link in tvlinks:\n",
    "    print(count)\n",
    "    count += 1\n",
    "    full_url = 'https://reelgood.com' + link\n",
    "    r = requests.get(full_url)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    n = len(soup.find('div',{'class':'css-1ss0qk ey4ir3j0'}).findAll('span',{'class':'css-ee2w7g ey4ir3j1'}))\n",
    "    num_of_spans.append(n)\n",
    "    sleep(0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(num_of_spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(driver.page_source, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find('div',{'class':'css-1ss0qk ey4ir3j0'}).findAll('span',{'class':'css-ee2w7g ey4ir3j1'})[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
