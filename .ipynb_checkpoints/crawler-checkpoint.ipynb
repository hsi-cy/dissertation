{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise the crawler\"\"\"\n",
    "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        \n",
    "    def get_page(self,url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "    \n",
    "    def scroll_down(self, t):\n",
    "        driver = self.driver\n",
    "        \n",
    "        sleep(0.5)\n",
    "        for i in range(t): \n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            sleep(0.6)\n",
    "    \n",
    "    def get_page_source(self):\n",
    "        driver = self.driver\n",
    "        return driver.page_source\n",
    "    \n",
    "    def get_title_links(self):\n",
    "        driver = self.driver\n",
    "        s = driver.page_source\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        source_titles = soup.findAll('a',{'class':'title'})\n",
    "        links = [title['href'] for title in source_titles]\n",
    "        return links\n",
    "    \n",
    "#     def flixable(self,content_type):\n",
    "#         \"\"\"This function will return a flixable url with parameters set\"\"\"\n",
    "\n",
    "#         required_type = ['tv-shows','movies']\n",
    "#         if content_type in required_type:\n",
    "#             headers = {\n",
    "#                         \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "\n",
    "#             url = f'https://flixable.com/netflix-originals/genre/{content_type}/#filterContainer'\n",
    "#             my_parameters = {'min-rating':0, 'min-year':1920, 'max-year':2019, 'order':'title'}\n",
    "\n",
    "#             re_url = requests.get(url , my_parameters,headers = headers).url\n",
    "#             return re_url\n",
    "\n",
    "#         else:\n",
    "#             print('KeyError. Please enter either tv-shows or movies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All netflix + All Disney+ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all netflix movies descriptions (US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://flixable.com/genre/movies'\n",
    "# parameters = {\n",
    "#     'max-year': '2019',\n",
    "#     'order': 'title'\n",
    "# }\n",
    "\n",
    "# res = requests.get(url, parameters, headers = HEADERS).url\n",
    "\n",
    "# source = crawler.get_page_source()\n",
    "# soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "# # get all links from movies\n",
    "# nMovieLinks = list()\n",
    "# count = 1\n",
    "# for ix in range(1,3796):\n",
    "#     if count == 100 or count == 500 or count == 1000 or count == 2000 or count == 3000:\n",
    "#         print(count)\n",
    "#     count += 1\n",
    "#     nMovieLinks.append(soup.find('div',{'id':ix}).find('a')['href'])\n",
    "\n",
    "# # Get all netflix movies info\n",
    "# netflix_all_movies_db = get_info('movie', nMovieLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_links(r, soup):\n",
    "    links = list()\n",
    "    count = 1\n",
    "    for ix in range(1, r):\n",
    "        if count%100 == 0 :\n",
    "            print(count)\n",
    "        count += 1\n",
    "        links.append(soup.find('div',{'id':ix}).find('a')['href'])\n",
    "    return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get All netflix tvshows descriptions (US)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "source = crawler.get_page_source()\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "nTvshowLinks = retrieve_links(1729, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n"
     ]
    }
   ],
   "source": [
    "netflix_all_tvshows_db = get_info('tvshow', nTvshowLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflix_all_tvshows_db.to_csv('netflixTvshowDb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all disneyplus movies info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "# get links and then get info\n",
    "source = crawler.get_page_source()\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "dMovieLinks = retrieve_links(768, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n"
     ]
    }
   ],
   "source": [
    "disneyPlus_all_movies_db = get_info_disney('movie', dMovieLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "disneyPlus_all_movies_db.to_csv('dMoviesDb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get all disney tvshows info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "source = crawler.get_page_source()\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "dTvshowLinks = retrieve_links(236, soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "disneyPlus_all_tvshows_db = get_info_disney('tvshow', dTvshowLinks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "disneyPlus_all_tvshows_db.to_csv('dTvshowsDb.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get title links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(url):\n",
    "    \"\"\"Return bs4 object\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_info(content_type, links):\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                               'runtime', 'description', 'genres', 'cast',\n",
    "                               'director', 'country', 'imdb_link', 'date_added'])\n",
    "    count = 1\n",
    "    for ix in range(len(links)):\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        \n",
    "        full_url = 'https://flixable.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        try:\n",
    "            release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "            rating = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "            runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[2].text\n",
    "        except:\n",
    "            release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "            rating = ''\n",
    "            runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "        description = soup.find('p',{'class':'card-description'}).text\n",
    "        tag_a = soup.find('div',{'class':'col-lg-8'}).findAll('a')\n",
    "        genres = list()\n",
    "        cast = list()\n",
    "        director = list()\n",
    "        country = list()\n",
    "        imdb_link = list()\n",
    "        for a in tag_a:\n",
    "            if 'genre' in a['href']:\n",
    "                genres.append(a.text)\n",
    "\n",
    "            elif 'director' in a['href']:\n",
    "                director.append(a.text)\n",
    "\n",
    "            elif 'actor' in a['href']:\n",
    "                cast.append(a.text)\n",
    "\n",
    "            elif 'country' in a['href']:\n",
    "                country.append(a.text)\n",
    "\n",
    "            elif 'www.imdb.com' in a['href']:\n",
    "                imdb_link.append(a['href'])\n",
    "        date_added = soup.find('div',{'class':'col-lg-8'}).findAll('p')[-1].text.lstrip().rstrip()\n",
    "        \n",
    "        sep = ','\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                                   'runtime', 'description', 'genres', 'cast',\n",
    "                                   'director', 'country', 'imdb_link', 'date_added'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'release_year': release_year,\n",
    "                             'rating': rating,\n",
    "                             'runtime': runtime,\n",
    "                             'description': description,\n",
    "                             'genres': sep.join(genres),\n",
    "                             'cast': sep.join(cast),\n",
    "                             'director': sep.join(director),\n",
    "                             'country': sep.join(country),\n",
    "                             'imdb_link': sep.join(imdb_link),\n",
    "                             'date_added': date_added\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        \n",
    "    return DF\n",
    "\n",
    "def get_info_disney(content_type, links):\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                               'runtime', 'description', 'genres', 'cast',\n",
    "                               'director', 'country', 'imdb_link', 'date_added'])\n",
    "    count = 1\n",
    "    for ix in range(len(links)):\n",
    "        if count % 100 == 0:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        \n",
    "        full_url = 'https://flixable.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        try:\n",
    "            release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "            rating = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "            runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[2].text\n",
    "        except:\n",
    "            release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "            rating = ''\n",
    "            runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "        try:\n",
    "            description = soup.find('p',{'id':'synopsis'}).text.lstrip().rstrip()\n",
    "        except:\n",
    "            description = soup.find('p',{'class':'card-description'}).text\n",
    "        tag_a = soup.find('div',{'class':'col-lg-8'}).findAll('a')\n",
    "        genres = list()\n",
    "        cast = list()\n",
    "        director = list()\n",
    "        country = list()\n",
    "        imdb_link = list()\n",
    "        for a in tag_a:\n",
    "            if 'genre' in a['href']:\n",
    "                genres.append(a.text)\n",
    "\n",
    "            elif 'director' in a['href']:\n",
    "                director.append(a.text)\n",
    "\n",
    "            elif 'actor' in a['href']:\n",
    "                cast.append(a.text)\n",
    "\n",
    "            elif 'country' in a['href']:\n",
    "                country.append(a.text)\n",
    "\n",
    "            elif 'www.imdb.com' in a['href']:\n",
    "                imdb_link.append(a['href'])\n",
    "        date_added = soup.find('div',{'class':'col-lg-8'}).findAll('p')[-1].text.lstrip().rstrip()\n",
    "        \n",
    "        sep = ','\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                                   'runtime', 'description', 'genres', 'cast',\n",
    "                                   'director', 'country', 'imdb_link', 'date_added'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'release_year': release_year,\n",
    "                             'rating': rating,\n",
    "                             'runtime': runtime,\n",
    "                             'description': description,\n",
    "                             'genres': sep.join(genres),\n",
    "                             'cast': sep.join(cast),\n",
    "                             'director': sep.join(director),\n",
    "                             'country': sep.join(country),\n",
    "                             'imdb_link': sep.join(imdb_link),\n",
    "                             'date_added': date_added\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_links = crawler.get_title_links()\n",
    "# movie_db = get_info('movies',movie_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = movie_db.append(tvshow_db, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timeit.default_timer()\n",
    "# tvshow_links = crawler.get_title_links()\n",
    "# tvshow_db = get_info('tvshow', tvshow_links)\n",
    "# end = timeit.default_timer()\n",
    "# print(start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rv_votes(dataset):\n",
    "    start = timeit.default_timer()\n",
    "    rating_value = list()\n",
    "    votes = list()\n",
    "    count = 1\n",
    "    for link in dataset['imdb_link']:\n",
    "        if count == 100 or count == 300 or count == 500 or count == 700 or count == 900 or count == 1200 or count == 1400:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        try:\n",
    "            soup = parser(link)\n",
    "            ratingValue = soup.find('span', {'itemprop':'ratingValue'}).text\n",
    "            ratingCount = soup.find('span', {'itemprop':'ratingCount'}).text\n",
    "            rating_value.append(ratingValue)\n",
    "            votes.append(ratingCount)\n",
    "        except:\n",
    "            rating_value.append('NaN')\n",
    "            votes.append('NaN')\n",
    "    \n",
    "    dataset['rating_value'] = rating_value\n",
    "    dataset['votes'] = votes\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    t = int((stop-start)/60)\n",
    "    print(f\"It took {t} minutes to scrape rating values and votes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "SPECIAL_CHARS = '[^A-Za-z0-9 ]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized = [word for sent in [re.sub(SPECIAL_CHARS, '', element).split(' ') for \n",
    "                                   element in nltk.sent_tokenize(text)] for word in sent]\n",
    "    lowered = [word.lower() for word in tokenized]\n",
    "    return lowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_texts = [preprocess(text) for text in dataset['description'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out the stopwords\n",
    "for i in range(len(description_texts)):\n",
    "    description_texts[i] = [word for word in description_texts[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use movie title as unique key. So I map out the title and the tokenised sentences\n",
    "\n",
    "title_text = dict(zip(dataset['title'].tolist(), description_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(description_texts, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['media']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = model.wv.most_similar('media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(first_map, second_map):\n",
    "    first_vec  = dict()\n",
    "    for uid, content in first_map.items():\n",
    "        temp = list()\n",
    "        for element in content:\n",
    "            try:\n",
    "                temp.append(second_map[element])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first_vec[uid] = np.mean(temp, axis=0)\n",
    "    \n",
    "    return first_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_vec = get_vectors(title_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(lookup_id):\n",
    "\n",
    "    sim = list()\n",
    "    \n",
    "    lookup_map = title_vec\n",
    "    subject_map = title_vec\n",
    "#     else:\n",
    "#         raise ValueError('Invalid value for parameter kind.')\n",
    "        \n",
    "    for uid, vec in lookup_map.items():\n",
    "        thisSim = cosine_similarity(vec.reshape(1, -1), subject_map[lookup_id].reshape(1, -1))\n",
    "        sim.append((uid, thisSim[0][0]))\n",
    "\n",
    "    return sorted(sim, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_similar(title):\n",
    "    \n",
    "    x = get_most_similar(title)[1:11]\n",
    "    for e in x:\n",
    "        rating = dataset.loc[dataset['title']==e[0]]['rating_value'].values.tolist()[0]\n",
    "        votes = dataset.loc[dataset['title']==e[0]]['votes'].values.tolist()[0]\n",
    "        print(f\"Movie title: {e[0]}\\nScores: {rating}\\nVotes: {votes}\\nSimilarity: {e[1]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_10_similar('Black Mirror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = dataset['title'] == 'Stranger Things'\n",
    "dataset.loc[filt]['votes'].values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
