{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "SPECIAL_CHARS = '[^A-Za-z0-9 ]+'\n",
    "STOP_WORDS = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Take out stopwords.\n",
    "    Take out punctuations and special characters.\n",
    "    \"\"\"\n",
    "    SPECIAL_CHARS = '[^A-Za-z0-9 ]+'\n",
    "    STOP_WORDS = stopwords.words('english')\n",
    "    text = text.lower().split(' ')\n",
    "    temp = [word for word in text if word not in STOP_WORDS]\n",
    "    text = ' '.join(temp)\n",
    "    text = re.sub(SPECIAL_CHARS, '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise(doc):\n",
    "    return [token.text for token in nlp(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_lemma(doc):\n",
    "    \"\"\"\n",
    "    Use spacy as the nlp object to tokenise each doc\n",
    "    Lemmatise each words\n",
    "    \"\"\"\n",
    "    return ' '.join([token.lemma_ for token in nlp(doc)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is one way to get each title's vector representation\n",
    "# more investagtion is needed later.\n",
    "\n",
    "def get_vectors(first_map, second_map):\n",
    "    \"\"\"\n",
    "    Use tokenised words to get vectors representations from the pretrained model (i.e. second_map).\n",
    "    Average the vector representation of the description as the representation of the document \n",
    "    (i.e. each movie title's representation is the mean of vectors of each words in its description)\n",
    "    \"\"\"\n",
    "    first_vec  = dict()\n",
    "    for title, description in first_map.items():\n",
    "        temp = list()\n",
    "        for element in description: #element = tokenised words\n",
    "            try:\n",
    "                temp.append(second_map[element]) #secondmap is w2v model which should have a responding word vecotr for the tokenise word\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first_vec[title] = np.mean(temp, axis=0)\n",
    "    \n",
    "    return first_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(name, df):\n",
    "    return df.loc[df['title'].str.lower()==name.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topN_similar(lookup_id, title_vec, df, N=10):\n",
    "\n",
    "    sim = list()\n",
    "    lookup_map = title_vec\n",
    "    subject_map = title_vec \n",
    "        \n",
    "    for uid, vec in lookup_map.items():\n",
    "        thisSim = cosine_similarity(vec.reshape(1, -1), subject_map[lookup_id].reshape(1, -1))\n",
    "        org = search(uid, df).originals.values\n",
    "        gen = search(uid, df).genres.values\n",
    "        sim.append((uid, thisSim[0][0], org, gen))\n",
    "    sim = sorted(sim, key=lambda x: x[1], reverse=True)[:N+1]\n",
    "    returnDf = pd.DataFrame(columns=['title','similarity','originals','genres'],\n",
    "                           data = sim)\n",
    "    return returnDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(lookup_id, title_vec, df):\n",
    "\n",
    "    sim = list()\n",
    "    lookup_map = title_vec\n",
    "    subject_map = title_vec \n",
    "        \n",
    "    for uid, vec in lookup_map.items():\n",
    "        thisSim = cosine_similarity(vec.reshape(1, -1), subject_map[lookup_id].reshape(1, -1))\n",
    "        org = search(uid, df).originals.values\n",
    "        gen = search(uid, df).genres.values\n",
    "        sim.append((uid, thisSim[0][0], org, gen))\n",
    "\n",
    "    return sorted(sim, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_df(keyword):\n",
    "    \"\"\"\n",
    "    Return a dataframe with the filtered result.\n",
    "    The input value is case-insensitive. \n",
    "    \"\"\"\n",
    "    if type(keyword) == list:\n",
    "        return netflixDf.loc[netflixDf['title'].isin(keyword)]\n",
    "    else:\n",
    "        return netflixDf.loc[netflixDf['title'].str.lower().isin([keyword.lower()])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markerX(key, values):\n",
    "    return netflixDf.loc[netflixDf[key].str.lower().isin(values)].sort_values(by='pca_2', ascending=False)\n",
    "\n",
    "def others(key, values):\n",
    "    return netflixDf.loc[~netflixDf[key].str.lower().isin(values)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analyse Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Terms explained\n",
    "Document -> a bunch of texts <br>\n",
    "Corpus -> a bunch of documents <br>\n",
    "Vectors -> a mathematically convenience representation of a document (a bunch of textx) <br>\n",
    "Models -> an algorithm for transforming vectors from one representation to another <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the dataset/ Load the spacy pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflixDf = pd.read_csv('finalDataset_v2.csv', usecols=['title','type','description','genres','originals'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned.csv', usecols=['cleaned'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "netflixDf = pd.concat([netflixDf,df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "title                                                                                                                                                                                                                  Lah\n",
       "genres                                                                                                                                                                                                         crime,drama\n",
       "description    Lah has one or more episodes streaming with subscription on Netflix. It's a crime and drama show with 24 episodes over 1 season. Lah is still airing with no announced date for the next episode or season.\n",
       "type                                                                                                                                                                                                                tvshow\n",
       "originals                                                                                                                                                                                                                0\n",
       "cleaned                                                                                                                                                                                                                NaN\n",
       "Name: 4633, dtype: object"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflixDf.iloc[4633]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 958, 2224, 3916, 3920, 3949, 3951, 4052, 4088, 4115, 4162, 4164,\n",
       "        4189, 4194, 4222, 4242, 4257, 4288, 4331, 4357, 4361, 4394, 4414,\n",
       "        4455, 4468, 4500, 4527, 4553, 4572, 4576, 4599, 4618, 4629, 4630,\n",
       "        4633, 4640, 4669, 4699, 4802, 4815, 4817, 4836, 4862, 4871, 4881,\n",
       "        4956, 5107, 5121, 5157, 5181, 5226, 5275, 5281, 5433, 5464, 5474,\n",
       "        5524, 5576, 5613, 5614, 5618, 5647, 5664, 5675, 5720, 5726, 5788]),)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netflixDf(pd.isnull(netflixDf.cleaned))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pre-trained corpus to help tokenise words\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse Descriptive Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieDf = netflixDf.loc[netflixDf['type']=='movie']\n",
    "tvshowDf = netflixDf.loc[netflixDf['type']=='tvshow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Corpus and apply word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieCorpus = movieDf.cleaned.values.tolist() #list of docs\n",
    "tvshowCorpus = tvshowDf.cleaned.values.tolist() #list of docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-109-9d6981e91c7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovieTkDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovieCorpus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#tokenise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtvshowTkDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtvshowCorpus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#tokenise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-109-9d6981e91c7a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmovieTkDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmovieCorpus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#tokenise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtvshowTkDocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtvshowCorpus\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#tokenise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-1e8c20cee533>\u001b[0m in \u001b[0;36mtokenise\u001b[0;34m(doc)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtokenise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/MSc/Dissertation/data/crawler/venv/lib/python3.8/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    432\u001b[0m         \u001b[0mDOCS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mapi\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;31m#call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \"\"\"\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m             raise ValueError(\n\u001b[1;32m    436\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "movieTkDocs = [tokenise(doc) for doc in movieCorpus] #tokenise \n",
    "tvshowTkDocs = [tokenise(doc) for doc in tvshowCorpus] #tokenise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping out the title and each description. so later on i can search \n",
    "movieMap = dict(zip(movieDf['title'].str.lower().tolist(), movieTkDocs))\n",
    "tvshowMap = dict(zip(tvshowDf['title'].str.lower().tolist(), tvshowTkDocs))\n",
    "# lower the title (easy for search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"GoogleNews-vectors-negative300.bin\"\n",
    "w2v = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "# It is much faster take less than 2 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movieTitleVec = get_vectors(movieMap, w2v)\n",
    "tvshowTitleVec = get_vectors(tvshowMap, w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_topN_similar('house of cards', tvshowTitleVec, tvshowDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_most_similar('house of cards', tvshowTitleVec, tvshowDf)[:15]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
