{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise the crawler\"\"\"\n",
    "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        \n",
    "    def get_page(self,url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "    \n",
    "    def scroll_down(self):\n",
    "        driver = self.driver\n",
    "        \n",
    "        sleep(0.5)\n",
    "        for i in range(20): \n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            sleep(0.6)\n",
    "    \n",
    "    def get_page_source(self):\n",
    "        driver = self.driver\n",
    "        return driver.page_source\n",
    "    \n",
    "    def get_title_links(self):\n",
    "        driver = self.driver\n",
    "        s = driver.page_source\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        source_titles = soup.findAll('a',{'class':'title'})\n",
    "        links = [title['href'] for title in source_titles]\n",
    "        return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flixable(content_type):\n",
    "    \"\"\"This function will return a flixable url with parameters set\"\"\"\n",
    "    required_type = ['tv-shows','movies']\n",
    "    if content_type in required_type:\n",
    "        headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "\n",
    "        url = f'https://flixable.com/netflix-originals/genre/{content_type}/#filterContainer'\n",
    "        my_parameters = {'min-rating':0, 'min-year':1920, 'max-year':2019, 'order':'title'}\n",
    "\n",
    "        re_url = requests.get(url , my_parameters,headers = headers).url\n",
    "        return re_url\n",
    "    \n",
    "    else:\n",
    "        print('KeyError. Please enter either tv-shows or movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# crawler = crawler()\n",
    "crawler.get_page('https://www.imdb.com/title/tt11194518/?ref_=plg_rt_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingValue = soup.find('span', {'itemprop':'ratingValue'}).text\n",
    "votes = soup.find('span', {'itemprop':'ratingCount'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get title links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_links = crawler.get_title_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(content_type, links):\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                               'runtime', 'description', 'genres', 'cast',\n",
    "                               'director', 'country', 'imdb_link', 'date_added'])\n",
    "    for ix in range(len(links)):\n",
    "        full_url = 'https://flixable.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "        rating = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "        runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[2].text\n",
    "        description = soup.find('p',{'class':'card-description'}).text\n",
    "        tag_a = soup.find('div',{'class':'col-lg-8'}).findAll('a')\n",
    "        genres = list()\n",
    "        cast = list()\n",
    "        director = list()\n",
    "        country = list()\n",
    "        imdb_link = list()\n",
    "        for a in tag_a:\n",
    "            if 'genre' in a['href']:\n",
    "                genres.append(a.text)\n",
    "\n",
    "            elif 'director' in a['href']:\n",
    "                director.append(a.text)\n",
    "\n",
    "            elif 'actor' in a['href']:\n",
    "                cast.append(a.text)\n",
    "\n",
    "            elif 'country' in a['href']:\n",
    "                country.append(a.text)\n",
    "\n",
    "            elif 'www.imdb.com' in a['href']:\n",
    "                imdb_link.append(a['href'])\n",
    "        date_added = soup.find('div',{'class':'col-lg-8'}).findAll('p')[-1].text.lstrip().rstrip()\n",
    "        \n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                                   'runtime', 'description', 'genres', 'cast',\n",
    "                                   'director', 'country', 'imdb_link', 'date_added'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'release_year': release_year,\n",
    "                             'rating': rating,\n",
    "                             'runtime': runtime,\n",
    "                             'description': description,\n",
    "                             'genres': genres,\n",
    "                             'cast': cast,\n",
    "                             'director': director,\n",
    "                             'country': country,\n",
    "                             'imdb_link': imdb_link,\n",
    "                             'date_added': date_added\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_db = get_info('movies',movie_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvshow_links = crawler.get_title_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvshow_db = get_info('tvshow', tvshow_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = movie_db.append(tvshow_db, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(url):\n",
    "    \"\"\"Return bs4 object\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratingValue = soup.find('span', {'itemprop':'ratingValue'}).text\n",
    "votes = soup.find('span', {'itemprop':'ratingCount'}).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1411, 12)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "322\n",
      "1274\n",
      "1349\n",
      "1350\n"
     ]
    }
   ],
   "source": [
    "for i in range(dataset.shape[0]):\n",
    "    if dataset['imdb_link'].iloc[i] == []:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "300\n",
      "500\n",
      "700\n",
      "900\n",
      "1200\n",
      "1400\n",
      "Time:  2323.908137589984\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "start = timeit.default_timer()\n",
    "rating_value = list()\n",
    "votes = list()\n",
    "count = 1\n",
    "for link in dataset['imdb_link']:\n",
    "    if count == 100 or count == 300 or count == 500 or count == 700 or count == 900 or count == 1200 or count == 1400:\n",
    "        print(count)\n",
    "    count += 1\n",
    "    try:\n",
    "        soup = parser(link[0])\n",
    "        ratingValue = soup.find('span', {'itemprop':'ratingValue'}).text\n",
    "        ratingCount = soup.find('span', {'itemprop':'ratingCount'}).text\n",
    "        rating_value.append(ratingValue)\n",
    "        votes.append(ratingCount)\n",
    "    except:\n",
    "        rating_value.append('NaN')\n",
    "        votes.append('NaN')\n",
    "        \n",
    "stop = timeit.default_timer()\n",
    "print('Time: ', stop - start) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['rating_value'] = rating_value\n",
    "dataset['votes'] = votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 38 minutes to scrape rating values and votes.\n"
     ]
    }
   ],
   "source": [
    "t = int(2323/60)\n",
    "print(f\"It took {t} minutes to scrape rating values and votes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
