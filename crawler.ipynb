{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/hsichengyun/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/hsichengyun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re, requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from time import sleep\n",
    "import timeit\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "import ssl\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "\n",
    "SPECIAL_CHARS = '[^A-Za-z0-9 ]+'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create crawler class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class crawler:\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialise the crawler\"\"\"\n",
    "        self.driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "        \n",
    "    def get_page(self,url):\n",
    "        driver = self.driver\n",
    "        driver.get(url)\n",
    "    \n",
    "    def scroll_down(self):\n",
    "        driver = self.driver\n",
    "        \n",
    "        sleep(0.5)\n",
    "        for i in range(20): \n",
    "            driver.execute_script('window.scrollTo(0, document.body.scrollHeight);')\n",
    "            sleep(0.6)\n",
    "    \n",
    "    def get_page_source(self):\n",
    "        driver = self.driver\n",
    "        return driver.page_source\n",
    "    \n",
    "    def get_title_links(self):\n",
    "        driver = self.driver\n",
    "        s = driver.page_source\n",
    "        soup = BeautifulSoup(s, 'html.parser')\n",
    "        source_titles = soup.findAll('a',{'class':'title'})\n",
    "        links = [title['href'] for title in source_titles]\n",
    "        return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "movies first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flixable(content_type):\n",
    "    \"\"\"This function will return a flixable url with parameters set\"\"\"\n",
    "    required_type = ['tv-shows','movies']\n",
    "    if content_type in required_type:\n",
    "        headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "\n",
    "        url = f'https://flixable.com/netflix-originals/genre/{content_type}/#filterContainer'\n",
    "        my_parameters = {'min-rating':0, 'min-year':1920, 'max-year':2019, 'order':'title'}\n",
    "\n",
    "        re_url = requests.get(url , my_parameters,headers = headers).url\n",
    "        return re_url\n",
    "    \n",
    "    else:\n",
    "        print('KeyError. Please enter either tv-shows or movies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crawler = crawler()\n",
    "url = flixable('tv-shows')\n",
    "crawler.get_page(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get title links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(url):\n",
    "    \"\"\"Return bs4 object\"\"\"\n",
    "    headers = {\"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/81.0.4044.129 Safari/537.36\"}\n",
    "    r = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_info(content_type, links):\n",
    "    DF = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                               'runtime', 'description', 'genres', 'cast',\n",
    "                               'director', 'country', 'imdb_link', 'date_added'])\n",
    "    for ix in range(len(links)):\n",
    "        full_url = 'https://flixable.com' + links[ix]\n",
    "        r = requests.get(full_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text\n",
    "        release_year = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[0].text\n",
    "        rating = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[1].text\n",
    "        runtime = soup.find('div',{'class':'col-lg-8'}).find('h6').findAll('span')[2].text\n",
    "        description = soup.find('p',{'class':'card-description'}).text\n",
    "        tag_a = soup.find('div',{'class':'col-lg-8'}).findAll('a')\n",
    "        genres = list()\n",
    "        cast = list()\n",
    "        director = list()\n",
    "        country = list()\n",
    "        imdb_link = list()\n",
    "        for a in tag_a:\n",
    "            if 'genre' in a['href']:\n",
    "                genres.append(a.text)\n",
    "\n",
    "            elif 'director' in a['href']:\n",
    "                director.append(a.text)\n",
    "\n",
    "            elif 'actor' in a['href']:\n",
    "                cast.append(a.text)\n",
    "\n",
    "            elif 'country' in a['href']:\n",
    "                country.append(a.text)\n",
    "\n",
    "            elif 'www.imdb.com' in a['href']:\n",
    "                imdb_link.append(a['href'])\n",
    "        date_added = soup.find('div',{'class':'col-lg-8'}).findAll('p')[-1].text.lstrip().rstrip()\n",
    "        \n",
    "        sep = ','\n",
    "        \n",
    "        df = pd.DataFrame(columns=['title', 'type', 'release_year', 'rating', \n",
    "                                   'runtime', 'description', 'genres', 'cast',\n",
    "                                   'director', 'country', 'imdb_link', 'date_added'],\n",
    "                         data = [{\n",
    "                             'title': title,\n",
    "                             'type': content_type,\n",
    "                             'release_year': release_year,\n",
    "                             'rating': rating,\n",
    "                             'runtime': runtime,\n",
    "                             'description': description,\n",
    "                             'genres': sep.join(genres),\n",
    "                             'cast': sep.join(cast),\n",
    "                             'director': sep.join(director),\n",
    "                             'country': sep.join(country),\n",
    "                             'imdb_link': sep.join(imdb_link),\n",
    "                             'date_added': date_added\n",
    "                         }])\n",
    "        DF = DF.append(df, ignore_index=True)\n",
    "        \n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# movie_links = crawler.get_title_links()\n",
    "# movie_db = get_info('movies',movie_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = movie_db.append(tvshow_db, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = timeit.default_timer()\n",
    "# tvshow_links = crawler.get_title_links()\n",
    "# tvshow_db = get_info('tvshow', tvshow_links)\n",
    "# end = timeit.default_timer()\n",
    "# print(start-end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_rv_votes(dataset):\n",
    "    start = timeit.default_timer()\n",
    "    rating_value = list()\n",
    "    votes = list()\n",
    "    count = 1\n",
    "    for link in dataset['imdb_link']:\n",
    "        if count == 100 or count == 300 or count == 500 or count == 700 or count == 900 or count == 1200 or count == 1400:\n",
    "            print(count)\n",
    "        count += 1\n",
    "        try:\n",
    "            soup = parser(link)\n",
    "            ratingValue = soup.find('span', {'itemprop':'ratingValue'}).text\n",
    "            ratingCount = soup.find('span', {'itemprop':'ratingCount'}).text\n",
    "            rating_value.append(ratingValue)\n",
    "            votes.append(ratingCount)\n",
    "        except:\n",
    "            rating_value.append('NaN')\n",
    "            votes.append('NaN')\n",
    "    \n",
    "    dataset['rating_value'] = rating_value\n",
    "    dataset['votes'] = votes\n",
    "    \n",
    "    stop = timeit.default_timer()\n",
    "    t = int((stop-start)/60)\n",
    "    print(f\"It took {t} minutes to scrape rating values and votes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokenized = [word for sent in [re.sub(SPECIAL_CHARS, '', element).split(' ') for \n",
    "                                   element in nltk.sent_tokenize(text)] for word in sent]\n",
    "    lowered = [word.lower() for word in tokenized]\n",
    "    return lowered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "description_texts = [preprocess(text) for text in dataset['description'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take out the stopwords\n",
    "for i in range(len(description_texts)):\n",
    "    description_texts[i] = [word for word in description_texts[i] if word not in stopwords.words('english')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use movie title as unique key. So I map out the title and the tokenised sentences\n",
    "\n",
    "title_text = dict(zip(dataset['title'].tolist(), description_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(description_texts, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['media']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar = model.wv.most_similar('media')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors(first_map, second_map):\n",
    "    first_vec  = dict()\n",
    "    for uid, content in first_map.items():\n",
    "        temp = list()\n",
    "        for element in content:\n",
    "            try:\n",
    "                temp.append(second_map[element])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        first_vec[uid] = np.mean(temp, axis=0)\n",
    "    \n",
    "    return first_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-87fc635d0a79>:7: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  temp.append(second_map[element])\n"
     ]
    }
   ],
   "source": [
    "title_vec = get_vectors(title_text, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_most_similar(lookup_id):\n",
    "\n",
    "    sim = list()\n",
    "    \n",
    "    lookup_map = title_vec\n",
    "    subject_map = title_vec\n",
    "#     else:\n",
    "#         raise ValueError('Invalid value for parameter kind.')\n",
    "        \n",
    "    for uid, vec in lookup_map.items():\n",
    "        thisSim = cosine_similarity(vec.reshape(1, -1), subject_map[lookup_id].reshape(1, -1))\n",
    "        sim.append((uid, thisSim[0][0]))\n",
    "\n",
    "    return sorted(sim, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_10_similar(title):\n",
    "    \n",
    "    x = get_most_similar(title)[1:11]\n",
    "    for e in x:\n",
    "        rating = dataset.loc[dataset['title']==e[0]]['rating_value'].values.tolist()[0]\n",
    "        votes = dataset.loc[dataset['title']==e[0]]['votes'].values.tolist()[0]\n",
    "        print(f\"Movie title: {e[0]}\\nScores: {rating}\\nVotes: {votes}\\nSimilarity: {e[1]}\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie title: Master of None\n",
      "Scores: 8.3\n",
      "Votes: 62,701\n",
      "Similarity: 0.7177394032478333\n",
      "\n",
      "Movie title: Terrace House: Aloha State\n",
      "Scores: 7.4\n",
      "Votes: 749\n",
      "Similarity: 0.705254077911377\n",
      "\n",
      "Movie title: Real Rob\n",
      "Scores: 6.4\n",
      "Votes: 2,297\n",
      "Similarity: 0.6997315883636475\n",
      "\n",
      "Movie title: Dark Tourist\n",
      "Scores: 7.6\n",
      "Votes: 5,353\n",
      "Similarity: 0.6988909244537354\n",
      "\n",
      "Movie title: See You Yesterday\n",
      "Scores: 5.1\n",
      "Votes: 8,154\n",
      "Similarity: 0.6935927867889404\n",
      "\n",
      "Movie title: The Confession Tapes\n",
      "Scores: 7.5\n",
      "Votes: 4,404\n",
      "Similarity: 0.6907021403312683\n",
      "\n",
      "Movie title: Motown Magic\n",
      "Scores: 7.9\n",
      "Votes: 234\n",
      "Similarity: 0.6896839141845703\n",
      "\n",
      "Movie title: COMEDIANS of the world\n",
      "Scores: 6.5\n",
      "Votes: 382\n",
      "Similarity: 0.6853635311126709\n",
      "\n",
      "Movie title: Felipe Neto: My Life Makes No Sense\n",
      "Scores: 4.4\n",
      "Votes: 181\n",
      "Similarity: 0.6822994947433472\n",
      "\n",
      "Movie title: The Sound of Your Heart\n",
      "Scores: 8.3\n",
      "Votes: 654\n",
      "Similarity: 0.6809266805648804\n",
      "\n"
     ]
    }
   ],
   "source": [
    "top_10_similar('Black Mirror')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'746,944'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filt = dataset['title'] == 'Stranger Things'\n",
    "dataset.loc[filt]['votes'].values.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
